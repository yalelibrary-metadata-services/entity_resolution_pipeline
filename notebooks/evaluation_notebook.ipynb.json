{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Resolution System Evaluation\n",
    "\n",
    "This notebook evaluates the performance of the entity resolution system. It loads the metrics, feature importance, test results, and other outputs from the pipeline and visualizes them for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score,\n",
    "    accuracy_score, roc_auc_score, confusion_matrix,\n",
    "    precision_recall_curve, roc_curve, auc\n",
    ")\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "\n",
    "# Set up a better default figure size\n",
    "plt.rcParams['figure.figsize'] = [12, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open('../config.yml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Set output directory\n",
    "output_dir = Path(config['system']['output_dir'])\n",
    "figures_dir = output_dir / 'figures'\n",
    "reports_dir = output_dir / 'reports'\n",
    "\n",
    "# Check if output directory exists\n",
    "if not output_dir.exists():\n",
    "    print(f\"Output directory {output_dir} does not exist. Run the pipeline first.\")\n",
    "else:\n",
    "    print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pipeline Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pipeline results\n",
    "pipeline_results_path = output_dir / 'pipeline_results.json'\n",
    "if pipeline_results_path.exists():\n",
    "    with open(pipeline_results_path, 'r') as f:\n",
    "        pipeline_results = json.load(f)\n",
    "    print(f\"Pipeline results loaded from {pipeline_results_path}\")\n",
    "else:\n",
    "    pipeline_results = {}\n",
    "    print(f\"Pipeline results not found at {pipeline_results_path}\")\n",
    "\n",
    "# Display pipeline results summary\n",
    "if pipeline_results:\n",
    "    print(\"\\nPipeline Results Summary:\\n\")\n",
    "    for stage, results in pipeline_results.items():\n",
    "        if isinstance(results, dict):\n",
    "            print(f\"Stage: {stage}\")\n",
    "            for key, value in results.items():\n",
    "                if isinstance(value, (int, float, str, bool)) or value is None:\n",
    "                    print(f\"  {key}: {value}\")\n",
    "            print()\n",
    "        elif stage == 'total_duration':\n",
    "            print(f\"Total Duration: {results:.2f} seconds\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load classification metrics\n",
    "metrics_path = output_dir / 'classification_metrics.json'\n",
    "if metrics_path.exists():\n",
    "    with open(metrics_path, 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    print(f\"Classification metrics loaded from {metrics_path}\")\n",
    "else:\n",
    "    metrics = {}\n",
    "    print(f\"Classification metrics not found at {metrics_path}\")\n",
    "\n",
    "# Display performance metrics\n",
    "if metrics:\n",
    "    print(\"\\nPerformance Metrics:\\n\")\n",
    "    for metric, value in metrics.items():\n",
    "        if metric not in ['confusion_matrix', 'feature_importance']:\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # Display confusion matrix\n",
    "    if 'confusion_matrix' in metrics:\n",
    "        cm = metrics['confusion_matrix']\n",
    "        print(\"\\nConfusion Matrix:\\n\")\n",
    "        print(f\"True Negatives: {cm['true_negatives']}\")\n",
    "        print(f\"False Positives: {cm['false_positives']}\")\n",
    "        print(f\"False Negatives: {cm['false_negatives']}\")\n",
    "        print(f\"True Positives: {cm['true_positives']}\")\n",
    "        \n",
    "        # Calculate derived metrics\n",
    "        total = cm['true_negatives'] + cm['false_positives'] + cm['false_negatives'] + cm['true_positives']\n",
    "        accuracy = (cm['true_negatives'] + cm['true_positives']) / total if total > 0 else 0\n",
    "        precision = cm['true_positives'] / (cm['true_positives'] + cm['false_positives']) if (cm['true_positives'] + cm['false_positives']) > 0 else 0\n",
    "        recall = cm['true_positives'] / (cm['true_positives'] + cm['false_negatives']) if (cm['true_positives'] + cm['false_negatives']) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        print(\"\\nDerived Metrics:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        # Create confusion matrix visualization\n",
    "        cm_array = np.array([\n",
    "            [cm['true_negatives'], cm['false_positives']],\n",
    "            [cm['false_negatives'], cm['true_positives']]\n",
    "        ])\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(\n",
    "            cm_array,\n",
    "            annot=True,\n",
    "            fmt='d',\n",
    "            cmap='Blues',\n",
    "            xticklabels=['Non-Match', 'Match'],\n",
    "            yticklabels=['Non-Match', 'Match']\n",
    "        )\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature importance\n",
    "if metrics and 'feature_importance' in metrics:\n",
    "    feature_importance = metrics['feature_importance']\n",
    "    \n",
    "    # Create DataFrame from feature importance\n",
    "    rows = []\n",
    "    for feature, details in feature_importance.items():\n",
    "        # Extract feature type from name (e.g., person_cosine -> cosine)\n",
    "        feature_type = feature.split('_')[-1] if '_' in feature else 'other'\n",
    "        \n",
    "        rows.append({\n",
    "            'feature': feature,\n",
    "            'weight': details['weight'],\n",
    "            'abs_weight': details['abs_weight'],\n",
    "            'importance': details['importance'],\n",
    "            'feature_type': feature_type\n",
    "        })\n",
    "    \n",
    "    df_importance = pd.DataFrame(rows)\n",
    "    \n",
    "    # Sort by importance\n",
    "    df_importance = df_importance.sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Display top features\n",
    "    print(\"Top 15 Features by Importance:\\n\")\n",
    "    display(df_importance.head(15))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Bar plot of feature importance\n",
    "    plt.subplot(2, 1, 1)\n",
    "    sns.barplot(x='importance', y='feature', data=df_importance.head(15), palette='viridis')\n",
    "    plt.title('Top 15 Features by Importance')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Bar plot of feature weights\n",
    "    plt.subplot(2, 1, 2)\n",
    "    sns.barplot(x='weight', y='feature', data=df_importance.head(15), palette='viridis')\n",
    "    plt.title('Top 15 Features by Weight')\n",
    "    plt.xlabel('Weight')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Group features by type\n",
    "    importance_by_type = df_importance.groupby('feature_type')['importance'].sum().reset_index()\n",
    "    importance_by_type = importance_by_type.sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Display importance by feature type\n",
    "    print(\"\\nFeature Importance by Type:\\n\")\n",
    "    display(importance_by_type)\n",
    "    \n",
    "    # Plot importance by feature type\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='importance', y='feature_type', data=importance_by_type, palette='viridis')\n",
    "    plt.title('Feature Importance by Type')\n",
    "    plt.xlabel('Total Importance')\n",
    "    plt.ylabel('Feature Type')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Export to Excel for further analysis\n",
    "    excel_path = output_dir / \"feature_importance_analysis.xlsx\"\n",
    "    with pd.ExcelWriter(excel_path) as writer:\n",
    "        df_importance.to_excel(writer, sheet_name='Feature Importance', index=False)\n",
    "        importance_by_type.to_excel(writer, sheet_name='Importance by Type', index=False)\n",
    "    \n",
    "    print(f\"\\nFeature importance analysis exported to {excel_path}\")\n",
    "else:\n",
    "    print(\"Feature importance not found in metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load detailed test results\n",
    "test_results_path = output_dir / 'test_results_detailed.csv'\n",
    "if test_results_path.exists():\n",
    "    test_df = pd.read_csv(test_results_path)\n",
    "    print(f\"Loaded {len(test_df)} test records from {test_results_path}\")\n",
    "    \n",
    "    # Display basic statistics\n",
    "    correct_count = len(test_df[test_df['correct']])\n",
    "    accuracy = correct_count / len(test_df)\n",
    "    \n",
    "    print(f\"\\nTest Results Summary:\")\n",
    "    print(f\"Total test instances: {len(test_df)}\")\n",
    "    print(f\"Correct predictions: {correct_count} ({accuracy:.2%})\")\n",
    "    print(f\"Incorrect predictions: {len(test_df) - correct_count} ({1-accuracy:.2%})\")\n",
    "    \n",
    "    # Display sample of test results\n",
    "    print(\"\\nSample Test Results:\")\n",
    "    display(test_df.sample(5))\n",
    "    \n",
    "    # Calculate basic metrics\n",
    "    try:\n",
    "        precision = precision_score(test_df['true_label'], test_df['predicted_label'])\n",
    "        recall = recall_score(test_df['true_label'], test_df['predicted_label'])\n",
    "        f1 = f1_score(test_df['true_label'], test_df['predicted_label'])\n",
    "        roc_auc = roc_auc_score(test_df['true_label'], test_df['confidence'])\n",
    "        \n",
    "        print(\"\\nMetrics from Test Results:\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating metrics: {e}\")\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    try:  \n",
    "        cm = confusion_matrix(test_df['true_label'], test_df['predicted_label'])\n",
    "        \n",
    "        # Create confusion matrix visualization\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(\n",
    "            cm,\n",
    "            annot=True,\n",
    "            fmt='d',\n",
    "            cmap='Blues',\n",
    "            xticklabels=['Non-Match', 'Match'],\n",
    "            yticklabels=['Non-Match', 'Match']\n",
    "        )\n",
    "        plt.title('Confusion Matrix from Test Results')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating confusion matrix: {e}\")\n",
    "    \n",
    "    # Analyze errors\n",
    "    error_df = test_df[~test_df['correct']]\n",
    "    false_positives = error_df[error_df['predicted_label'] == 1]\n",
    "    false_negatives = error_df[error_df['predicted_label'] == 0]\n",
    "    \n",
    "    print(\"\\nError Analysis:\")\n",
    "    print(f\"Total errors: {len(error_df)}\")\n",
    "    print(f\"False positives: {len(false_positives)}\")\n",
    "    print(f\"False negatives: {len(false_negatives)}\")\n",
    "    \n",
    "    # Plot confidence distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create a new column for error type\n",
    "    test_df['result'] = 'Correct'\n",
    "    test_df.loc[(test_df['true_label'] == 0) & (test_df['predicted_label'] == 1), 'result'] = 'False Positive'\n",
    "    test_df.loc[(test_df['true_label'] == 1) & (test_df['predicted_label'] == 0), 'result'] = 'False Negative'\n",
    "    \n",
    "    # Plot confidence distribution by result type\n",
    "    sns.histplot(\n",
    "        data=test_df,\n",
    "        x='confidence',\n",
    "        hue='result',\n",
    "        multiple='stack',\n",
    "        bins=20\n",
    "    )\n",
    "    plt.title('Confidence Distribution by Result Type')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Count')\n",
    "    plt.axvline(x=0.9, color='red', linestyle='--', label='Default Threshold (0.9)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create precision-recall curve\n",
    "    try:\n",
    "        precision, recall, thresholds = precision_recall_curve(test_df['true_label'], test_df['confidence'])\n",
    "        \n",
    "        # Calculate F1 scores for each threshold\n",
    "        f1_scores = [2 * p * r / (p + r) if p + r > 0 else 0 for p, r in zip(precision[:-1], recall[:-1])]\n",
    "        best_threshold_idx = np.argmax(f1_scores)\n",
    "        best_threshold = thresholds[best_threshold_idx]\n",
    "        best_f1 = f1_scores[best_threshold_idx]\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(recall, precision, 'b-', label='Precision-Recall curve')\n",
    "        plt.scatter(recall[best_threshold_idx], precision[best_threshold_idx], color='red', s=100, \n",
    "                    label=f'Best F1 Threshold ({best_threshold:.2f}, F1={best_f1:.2f})')\n",
    "        \n",
    "        # Find closest point to current threshold\n",
    "        current_threshold = 0.9  # Default threshold\n",
    "        current_idx = np.argmin(np.abs(thresholds - current_threshold))\n",
    "        plt.scatter(recall[current_idx], precision[current_idx], color='green', s=100, \n",
    "                    label=f'Current Threshold ({current_threshold:.2f})')\n",
    "        \n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall Curve')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot threshold analysis\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(thresholds, precision[:-1], 'b-', label='Precision')\n",
    "        plt.plot(thresholds, recall[:-1], 'g-', label='Recall')\n",
    "        plt.plot(thresholds, f1_scores, 'r-', label='F1 Score')\n",
    "        plt.axvline(x=best_threshold, color='red', linestyle='--', label=f'Best F1 Threshold ({best_threshold:.2f})')\n",
    "        plt.axvline(x=current_threshold, color='green', linestyle='--', label=f'Current Threshold ({current_threshold:.2f})')\n",
    "        plt.xlabel('Threshold')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Precision, Recall, and F1 Score vs Threshold')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating precision-recall curve: {e}\")\n",
    "    \n",
    "    # Feature analysis for errors\n",
    "    try:\n",
    "        # Get feature columns (excluding metadata columns)\n",
    "        feature_cols = [col for col in test_df.columns if col not in ['pair_id', 'left_id', 'right_id', 'true_label', 'predicted_label', 'confidence', 'correct', 'result']]\n",
    "        \n",
    "        if len(feature_cols) > 0:\n",
    "            # Calculate average feature values for each result type\n",
    "            feature_means = test_df.groupby('result')[feature_cols].mean()\n",
    "            \n",
    "            print(\"\\nAverage Feature Values by Result Type:\")\n",
    "            display(feature_means)\n",
    "            \n",
    "            # Create barplot for top 5 features with highest variance across result types\n",
    "            feature_variances = feature_means.var(axis=0).sort_values(ascending=False)\n",
    "            top_features = feature_variances.index[:5].tolist()\n",
    "            \n",
    "            # Melt the dataframe for easier plotting\n",
    "            feature_means_reset = feature_means.reset_index()\n",
    "            feature_means_melted = pd.melt(feature_means_reset, id_vars=['result'], value_vars=top_features)\n",
    "            \n",
    "            plt.figure(figsize=(15, 8))\n",
    "            sns.barplot(x='variable', y='value', hue='result', data=feature_means_melted)\n",
    "            plt.title('Top Features by Result Type')\n",
    "            plt.xlabel('Feature')\n",
    "            plt.ylabel('Average Value')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.legend(title='Result')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Create boxplots for top features to see distributions\n",
    "            for feature in top_features:\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                sns.boxplot(x='result', y=feature, data=test_df)\n",
    "                plt.title(f'Distribution of {feature} by Result Type')\n",
    "                plt.xlabel('Result')\n",
    "                plt.ylabel(feature)\n",
    "                plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing features: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Export error analysis for detailed review\n",
    "    try:\n",
    "        error_analysis_path = output_dir / \"error_analysis_detailed.xlsx\"\n",
    "        with pd.ExcelWriter(error_analysis_path) as writer:\n",
    "            false_positives.to_excel(writer, sheet_name='False Positives', index=False)\n",
    "            false_negatives.to_excel(writer, sheet_name='False Negatives', index=False)\n",
    "            \n",
    "            # Include feature means\n",
    "            if len(feature_cols) > 0:\n",
    "                feature_means.to_excel(writer, sheet_name='Feature Means')\n",
    "                \n",
    "                # Create feature comparison for errors vs correct predictions\n",
    "                feature_comparison = pd.DataFrame()\n",
    "                for feature in feature_cols:\n",
    "                    feature_comparison[feature] = [\n",
    "                        test_df[test_df['correct']][feature].mean(),\n",
    "                        false_positives[feature].mean(),\n",
    "                        false_negatives[feature].mean()\n",
    "                    ]\n",
    "                \n",
    "                feature_comparison.index = ['Correct', 'False Positive', 'False Negative']\n",
    "                feature_comparison.to_excel(writer, sheet_name='Feature Comparison')\n",
    "        \n",
    "        print(f\"\\nError analysis exported to {error_analysis_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting analysis: {e}\")\n",
    "else:\n",
    "    print(f\"Test results not found at {test_results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clusters CSV (preferred format for analysis)\n",
    "clusters_csv_path = output_dir / 'clusters.csv'\n",
    "if clusters_csv_path.exists():\n",
    "    clusters_df = pd.read_csv(clusters_csv_path)\n",
    "    print(f\"Loaded {len(clusters_df)} cluster records from {clusters_csv_path}\")\n",
    "    \n",
    "    # Basic cluster statistics\n",
    "    cluster_count = clusters_df['cluster_id'].nunique()\n",
    "    entity_count = len(clusters_df)\n",
    "    avg_cluster_size = entity_count / cluster_count if cluster_count > 0 else 0\n",
    "    \n",
    "    print(f\"\\nCluster Statistics:\")\n",
    "    print(f\"Total clusters: {cluster_count}\")\n",
    "    print(f\"Total entities in clusters: {entity_count}\")\n",
    "    print(f\"Average entities per cluster: {avg_cluster_size:.2f}\")\n",
    "    \n",
    "    # Cluster size distribution\n",
    "    cluster_sizes = clusters_df.groupby('cluster_id').size().reset_index(name='size')\n",
    "    \n",
    "    print(\"\\nCluster Size Distribution:\")\n",
    "    size_counts = {\n",
    "        '1': sum(1 for size in cluster_sizes['size'] if size == 1),\n",
    "        '2': sum(1 for size in cluster_sizes['size'] if size == 2),\n",
    "        '3-5': sum(1 for size in cluster_sizes['size'] if 3 <= size <= 5),\n",
    "        '6-10': sum(1 for size in cluster_sizes['size'] if 6 <= size <= 10),\n",
    "        '11-20': sum(1 for size in cluster_sizes['size'] if 11 <= size <= 20),\n",
    "        '21+': sum(1 for size in cluster_sizes['size'] if size > 20)\n",
    "    }\n",
    "    \n",
    "    for size_range, count in size_counts.items():\n",
    "        print(f\"{size_range}: {count} clusters ({count/cluster_count:.2%})\")\n",
    "    \n",
    "    # Plot cluster size histogram\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(cluster_sizes['size'], bins=20, color='skyblue', edgecolor='black')\n",
    "    plt.title('Histogram of Cluster Sizes')\n",
    "    plt.xlabel('Cluster Size')\n",
    "    plt.ylabel('Number of Clusters')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot cluster size distribution (bar chart)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(size_counts.keys(), size_counts.values(), color='skyblue')\n",
    "    plt.title('Cluster Size Distribution')\n",
    "    plt.xlabel('Cluster Size Range')\n",
    "    plt.ylabel('Number of Clusters')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.show()\n",
    "    \n",
    "    # Show top 10 largest clusters\n",
    "    top_clusters = cluster_sizes.sort_values('size', ascending=False).head(10)\n",
    "    \n",
    "    print(\"\\nTop 10 Largest Clusters:\")\n",
    "    display(top_clusters)\n",
    "    \n",
    "    # Plot top 10 largest clusters\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='cluster_id', y='size', data=top_clusters)\n",
    "    plt.title('Top 10 Largest Clusters')\n",
    "    plt.xlabel('Cluster ID')\n",
    "    plt.ylabel('Number of Entities')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.show()\n",
    "else:\n",
    "    # Fall back to JSON file\n",
    "    clusters_path = output_dir / 'clusters.json'\n",
    "    if clusters_path.exists():\n",
    "        with open(clusters_path, 'r') as f:\n",
    "            clusters = json.load(f)\n",
    "        \n",
    "        print(f\"Loaded {len(clusters)} clusters from {clusters_path}\")\n",
    "        \n",
    "        # Calculate cluster sizes\n",
    "        cluster_sizes = [len(cluster) for cluster in clusters]\n",
    "        \n",
    "        # Display basic statistics\n",
    "        print(f\"\\nCluster Statistics:\")\n",
    "        print(f\"Total clusters: {len(clusters)}\")\n",
    "        print(f\"Total entities in clusters: {sum(cluster_sizes)}\")\n",
    "        print(f\"Average entities per cluster: {np.mean(cluster_sizes):.2f}\")\n",
    "        print(f\"Median entities per cluster: {np.median(cluster_sizes):.2f}\")\n",
    "        print(f\"Largest cluster size: {max(cluster_sizes)}\")\n",
    "        print(f\"Smallest cluster size: {min(cluster_sizes)}\")\n",
    "        \n",
    "        # Create cluster size distribution\n",
    "        print(\"\\nCluster Size Distribution:\")\n",
    "        size_counts = {\n",
    "            '1': sum(1 for size in cluster_sizes if size == 1),\n",
    "            '2': sum(1 for size in cluster_sizes if size == 2),\n",
    "            '3-5': sum(1 for size in cluster_sizes if 3 <= size <= 5),\n",
    "            '6-10': sum(1 for size in cluster_sizes if 6 <= size <= 10),\n",
    "            '11-20': sum(1 for size in cluster_sizes if 11 <= size <= 20),\n",
    "            '21+': sum(1 for size in cluster_sizes if size > 20)\n",
    "        }\n",
    "        \n",
    "        for size_range, count in size_counts.items():\n",
    "            print(f\"{size_range}: {count} clusters ({count/len(clusters):.2%})\")\n",
    "        \n",
    "        # Plot cluster size histogram\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.hist(cluster_sizes, bins=20, color='skyblue', edgecolor='black')\n",
    "        plt.title('Histogram of Cluster Sizes')\n",
    "        plt.xlabel('Cluster Size')\n",
    "        plt.ylabel('Number of Clusters')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Clusters not found at {clusters_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Analysis (Full Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prediction results\n",
    "prediction_csv_path = output_dir / 'prediction_results.csv'\n",
    "if prediction_csv_path.exists():\n",
    "    predictions_df = pd.read_csv(prediction_csv_path)\n",
    "    print(f\"Loaded {len(predictions_df)} prediction records from {prediction_csv_path}\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    match_count = len(predictions_df[predictions_df['predicted_match']])\n",
    "    non_match_count = len(predictions_df[~predictions_df['predicted_match']])\n",
    "    match_percentage = match_count / len(predictions_df) if len(predictions_df) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nPrediction Statistics:\")\n",
    "    print(f\"Total predictions: {len(predictions_df)}\")\n",
    "    print(f\"Predicted matches: {match_count} ({match_percentage:.2%})\")\n",
    "    print(f\"Predicted non-matches: {non_match_count} ({1-match_percentage:.2%})\")\n",
    "    \n",
    "    # Display sample of predictions\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    display(predictions_df.sample(5))\n",
    "    \n",
    "    # Plot confidence distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(\n",
    "        data=predictions_df,\n",
    "        x='confidence',\n",
    "        hue='predicted_match',\n",
    "        multiple='stack',\n",
    "        bins=20\n",
    "    )\n",
    "    plt.title('Confidence Distribution by Prediction')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Count')\n",
    "    plt.axvline(x=0.9, color='red', linestyle='--', label='Decision Threshold (0.9)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze average feature values for matches vs non-matches\n",
    "    # Get feature columns (excluding metadata columns)\n",
    "    feature_cols = [col for col in predictions_df.columns if col not in ['pair_id', 'left_id', 'right_id', 'predicted_match', 'confidence']]\n",
    "    \n",
    "    if len(feature_cols) > 0:\n",
    "        # Calculate average feature values for matches and non-matches\n",
    "        feature_means = predictions_df.groupby('predicted_match')[feature_cols].mean()\n",
    "        \n",
    "        print(\"\\nAverage Feature Values by Prediction:\")\n",
    "        display(feature_means)\n",
    "        \n",
    "        # Create barplot for top 5 features with highest variance between match and non-match\n",
    "        feature_variances = feature_means.var(axis=0).sort_values(ascending=False)\n",
    "        top_features = feature_variances.index[:5].tolist()\n",
    "        \n",
    "        # Melt the dataframe for easier plotting\n",
    "        feature_means_reset = feature_means.reset_index()\n",
    "        feature_means_melted = pd.melt(feature_means_reset, id_vars=['predicted_match'], value_vars=top_features)\n",
    "        \n",
    "        plt.figure(figsize=(15, 8))\n",
    "        sns.barplot(x='variable', y='value', hue='predicted_match', data=feature_means_melted)\n",
    "        plt.title('Top Features by Prediction')\n",
    "        plt.xlabel('Feature')\n",
    "        plt.ylabel('Average Value')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend(title='Predicted Match')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot distributions for all top features\n",
    "        for feature in top_features:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.histplot(\n",
    "                data=predictions_df,\n",
    "                x=feature,\n",
    "                hue='predicted_match',\n",
    "                multiple='layer',\n",
    "                bins=20\n",
    "            )\n",
    "            plt.title(f'Distribution of {feature} by Prediction')\n",
    "            plt.xlabel(feature)\n",
    "            plt.ylabel('Count')\n",
    "            plt.legend(title='Predicted Match')\n",
    "            plt.show()\n",
    "    \n",
    "    # Export prediction analysis\n",
    "    prediction_analysis_path = output_dir / \"prediction_analysis.xlsx\"\n",
    "    with pd.ExcelWriter(prediction_analysis_path) as writer:\n",
    "        # Export match and non-match predictions separately\n",
    "        predictions_df[predictions_df['predicted_match']].to_excel(writer, sheet_name='Matches', index=False)\n",
    "        predictions_df[~predictions_df['predicted_match']].to_excel(writer, sheet_name='Non-Matches', index=False)\n",
    "        \n",
    "        # Export feature means\n",
    "        if len(feature_cols) > 0:\n",
    "            feature_means.to_excel(writer, sheet_name='Feature Means')\n",
    "    \n",
    "    print(f\"\\nPrediction analysis exported to {prediction_analysis_path}\")\n",
    "else:\n",
    "    # Fall back to JSON file\n",
    "    predictions_path = output_dir / 'predictions.json'\n",
    "    if predictions_path.exists():\n",
    "        with open(predictions_path, 'r') as f:\n",
    "            predictions = json.load(f)\n",
    "        \n",
    "        print(f\"Loaded {len(predictions)} predictions from {predictions_path}\")\n",
    "        \n",
    "        # Calculate basic statistics\n",
    "        match_count = sum(1 for p in predictions.values() if p.get('match', False))\n",
    "        non_match_count = sum(1 for p in predictions.values() if not p.get('match', False))\n",
    "        match_percentage = match_count / len(predictions) if len(predictions) > 0 else 0\n",
    "        \n",
    "        print(f\"\\nPrediction Statistics:\")\n",
    "        print(f\"Total predictions: {len(predictions)}\")\n",
    "        print(f\"Predicted matches: {match_count} ({match_percentage:.2%})\")\n",
    "        print(f\"Predicted non-matches: {non_match_count} ({1-match_percentage:.2%})\")\n",
    "        \n",
    "        # Extract confidence values\n",
    "        confidence_values = [p.get('probability', 0) for p in predictions.values()]\n",
    "        \n",
    "        # Plot confidence distribution\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.hist(confidence_values, bins=20, color='skyblue', edgecolor='black')\n",
    "        plt.title('Confidence Distribution')\n",
    "        plt.xlabel('Confidence')\n",
    "        plt.ylabel('Count')\n",
    "        plt.axvline(x=0.9, color='red', linestyle='--', label='Decision Threshold (0.9)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Predictions not found at {predictions_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a performance summary with evaluation insights\n",
    "\n",
    "# Start with an empty summary\n",
    "summary = {\n",
    "    'classification': {},\n",
    "    'features': {},\n",
    "    'clusters': {},\n",
    "    'runtime': {}\n",
    "}\n",
    "\n",
    "# Add classification metrics\n",
    "if metrics:\n",
    "    for metric, value in metrics.items():\n",
    "        if metric not in ['confusion_matrix', 'feature_importance']:\n",
    "            summary['classification'][metric] = value\n",
    "    \n",
    "    if 'confusion_matrix' in metrics:\n",
    "        cm = metrics['confusion_matrix']\n",
    "        total = cm['true_negatives'] + cm['false_positives'] + cm['false_negatives'] + cm['true_positives']\n",
    "        summary['classification']['accuracy'] = (cm['true_negatives'] + cm['true_positives']) / total if total > 0 else 0\n",
    "        summary['classification']['precision'] = cm['true_positives'] / (cm['true_positives'] + cm['false_positives']) if (cm['true_positives'] + cm['false_positives']) > 0 else 0\n",
    "        summary['classification']['recall'] = cm['true_positives'] / (cm['true_positives'] + cm['false_negatives']) if (cm['true_positives'] + cm['false_negatives']) > 0 else 0\n",
    "        summary['classification']['f1'] = 2 * summary['classification']['precision'] * summary['classification']['recall'] / (summary['classification']['precision'] + summary['classification']['recall']) if (summary['classification']['precision'] + summary['classification']['recall']) > 0 else 0\n",
    "\n",
    "# Add feature importance info\n",
    "if metrics and 'feature_importance' in metrics:\n",
    "    feature_importance = metrics['feature_importance']\n",
    "    \n",
    "    # Get top 5 features\n",
    "    top_features = list(feature_importance.keys())[:5]\n",
    "    summary['features']['top_features'] = top_features\n",
    "    \n",
    "    # Group by feature type\n",
    "    feature_types = {}\n",
    "    for feature_name, details in feature_importance.items():\n",
    "        feature_type = feature_name.split('_')[-1] if '_' in feature_name else 'other'\n",
    "        if feature_type not in feature_types:\n",
    "            feature_types[feature_type] = 0\n",
    "        feature_types[feature_type] += details['importance']\n",
    "    \n",
    "    # Sort by importance\n",
    "    feature_types = {k: v for k, v in sorted(feature_types.items(), key=lambda item: item[1], reverse=True)}\n",
    "    \n",
    "    # Add top 3 feature types\n",
    "    summary['features']['top_feature_types'] = list(feature_types.keys())[:3]\n",
    "\n",
    "# Add cluster statistics\n",
    "clusters_csv_path = output_dir / 'clusters.csv'\n",
    "if clusters_csv_path.exists():\n",
    "    clusters_df = pd.read_csv(clusters_csv_path)\n",
    "    \n",
    "    # Basic cluster statistics\n",
    "    cluster_count = clusters_df['cluster_id'].nunique()\n",
    "    entity_count = len(clusters_df)\n",
    "    avg_cluster_size = entity_count / cluster_count if cluster_count > 0 else 0\n",
    "    \n",
    "    summary['clusters']['total_clusters'] = cluster_count\n",
    "    summary['clusters']['total_entities'] = entity_count\n",
    "    summary['clusters']['avg_cluster_size'] = avg_cluster_size\n",
    "    \n",
    "    # Get largest clusters\n",
    "    cluster_sizes = clusters_df.groupby('cluster_id').size().reset_index(name='size')\n",
    "    largest_cluster = cluster_sizes['size'].max() if not cluster_sizes.empty else 0\n",
    "    summary['clusters']['largest_cluster_size'] = largest_cluster\n",
    "\n",
    "# Add runtime statistics\n",
    "if pipeline_results:\n",
    "    for stage, results in pipeline_results.items():\n",
    "        if isinstance(results, dict) and 'duration' in results:\n",
    "            summary['runtime'][stage] = results['duration']\n",
    "    \n",
    "    if 'total_duration' in pipeline_results:\n",
    "        summary['runtime']['total'] = pipeline_results['total_duration']\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n===== ENTITY RESOLUTION SYSTEM PERFORMANCE SUMMARY =====\\n\")\n",
    "\n",
    "# Classification performance\n",
    "print(\"CLASSIFICATION PERFORMANCE:\")\n",
    "if summary['classification']:\n",
    "    for metric, value in summary['classification'].items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            print(f\"  {metric.capitalize()}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {metric.capitalize()}: {value}\")\n",
    "else:\n",
    "    print(\"  No classification metrics available\")\n",
    "\n",
    "# Feature importance\n",
    "print(\"\\nFEATURE IMPORTANCE:\")\n",
    "if 'top_features' in summary['features']:\n",
    "    print(\"  Top 5 Features:\")\n",
    "    for i, feature in enumerate(summary['features']['top_features'], 1):\n",
    "        importance = feature_importance[feature]['importance']\n",
    "        print(f\"    {i}. {feature} ({importance:.4f})\")\n",
    "    \n",
    "    print(\"\\n  Top Feature Types:\")\n",
    "    for i, feature_type in enumerate(summary['features']['top_feature_types'], 1):\n",
    "        print(f\"    {i}. {feature_type}\")\n",
    "else:\n",
    "    print(\"  No feature importance information available\")\n",
    "\n",
    "# Cluster statistics\n",
    "print(\"\\nCLUSTER STATISTICS:\")\n",
    "if summary['clusters']:\n",
    "    print(f\"  Total Clusters: {summary['clusters']['total_clusters']}\")\n",
    "    print(f\"  Total Entities: {summary['clusters']['total_entities']}\")\n",
    "    print(f\"  Average Cluster Size: {summary['clusters']['avg_cluster_size']:.2f}\")\n",
    "    print(f\"  Largest Cluster Size: {summary['clusters']['largest_cluster_size']}\")\n",
    "else:\n",
    "    print(\"  No cluster statistics available\")\n",
    "\n",
    "# Runtime statistics\n",
    "print(\"\\nRUNTIME STATISTICS:\")\n",
    "if summary['runtime']:\n",
    "    for stage, duration in summary['runtime'].items():\n",
    "        if stage != 'total':\n",
    "            print(f\"  {stage.capitalize()}: {duration:.2f} seconds\")\n",
    "    \n",
    "    if 'total' in summary['runtime']:\n",
    "        print(f\"\\n  Total Runtime: {summary['runtime']['total']:.2f} seconds\")\n",
    "else:\n",
    "    print(\"  No runtime statistics available\")\n",
    "\n",
    "print(\"\\n===== CONCLUSIONS =====\\n\")\n",
    "\n",
    "# Provide some insights based on results\n",
    "insights = []\n",
    "\n",
    "# Classification performance insights\n",
    "if 'precision' in summary['classification'] and 'recall' in summary['classification']:\n",
    "    precision = summary['classification']['precision']\n",
    "    recall = summary['classification']['recall']\n",
    "    \n",
    "    if precision > 0.9 and recall > 0.9:\n",
    "        insights.append(\"The model shows excellent balance between precision and recall, indicating high-quality entity resolution.\")\n",
    "    elif precision > 0.9 and recall < 0.8:\n",
    "        insights.append(\"The model prioritizes precision over recall, which is appropriate for applications requiring high confidence in matches.\")\n",
    "    elif precision < 0.8 and recall > 0.9:\n",
    "        insights.append(\"The model prioritizes recall over precision, which is suitable for applications where finding all potential matches is critical.\")\n",
    "    elif precision < 0.7 or recall < 0.7:\n",
    "        insights.append(\"The model shows room for improvement in classification performance. Consider refining features or model parameters.\")\n",
    "\n",
    "# Feature importance insights\n",
    "if 'top_features' in summary['features'] and 'top_feature_types' in summary['features']:\n",
    "    top_feature = summary['features']['top_features'][0]\n",
    "    top_type = summary['features']['top_feature_types'][0]\n",
    "    \n",
    "    insights.append(f\"The most influential feature is '{top_feature}', and the most important feature type is '{top_type}'.\")\n",
    "    \n",
    "    if 'cosine' in top_type:\n",
    "        insights.append(\"Vector similarity (cosine) features are dominant, indicating that the embedding model is effectively capturing semantic relationships.\")\n",
    "    elif 'jaro_winkler' in top_type or 'levenshtein' in top_type:\n",
    "        insights.append(\"String similarity features are dominant, suggesting that direct text comparisons are more effective than semantic embeddings for this dataset.\")\n",
    "    elif 'harmonic' in top_type:\n",
    "        insights.append(\"Interaction features (harmonic means) are dominant, highlighting the importance of considering relationships between different fields.\")\n",
    "\n",
    "# Cluster insights\n",
    "if 'total_clusters' in summary['clusters'] and 'avg_cluster_size' in summary['clusters']:\n",
    "    avg_size = summary['clusters']['avg_cluster_size']\n",
    "    \n",
    "    if avg_size < 2:\n",
    "        insights.append(\"The majority of clusters contain only a single entity, indicating either high precision in entity resolution or a dataset with few true duplicates.\")\n",
    "    elif avg_size > 5:\n",
    "        insights.append(\"The relatively large average cluster size suggests either many duplicate records per entity or potential over-merging in the clustering algorithm.\")\n",
    "\n",
    "# Print insights\n",
    "for insight in insights:\n",
    "    print(f\"• {insight}\")\n",
    "\n",
    "if not insights:\n",
    "    print(\"Insufficient data for generating insights. Run the complete pipeline to get more comprehensive insights.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
