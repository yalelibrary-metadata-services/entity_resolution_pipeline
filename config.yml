##############################
# Entity Resolution Pipeline Configuration
##############################

# System Configuration
system:
  mode: "dev"  # "dev" or "prod"
  log_level: "DEBUG"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  checkpoint_dir: "./checkpoints"
  output_dir: "./output"
  temp_dir: "./tmp"
  max_workers: 8  # Number of worker processes for parallel processing
  batch_size: 1000  # Records per batch
  dev_sample_size: 10000  # Number of records to process in dev mode
  random_seed: 42
  memory_limit_gb: 24  # Maximum memory usage in GB (for dev: 24GB, for prod: 240GB)

# Data Paths
data:
  input_dir: "./data/input"
  ground_truth_file: "./data/ground_truth/labeled_matches.csv"
  train_test_split: 0.7  # 70% training, 30% testing
  checkpoints_enabled: true
  checkpoint_frequency: 10000  # Save checkpoint every N records

# Embedding Configuration
embedding:
  model: "text-embedding-3-small"  # OpenAI model to use
  dimensions: 1536
  batch_size: 100  # Number of strings to embed in each API call
  request_timeout: 60  # Timeout for embedding API requests in seconds
  rpm_limit: 10000  # Rate limit: requests per minute
  tpm_limit: 5000000  # Rate limit: tokens per minute
  daily_token_limit: 500000000  # Daily token limit
  retry_attempts: 3
  retry_delay: 5  # Seconds
  fields_to_embed:
    - "composite"
    - "person"
    - "title"
    - "provision"
    - "subjects"

# Weaviate Configuration
weaviate:
  host: "localhost"
  port: 8080
  batch_size: 100
  timeout: 300  # Connection timeout in seconds
  collection_name: "UniqueStringsByField"
  ef: 128  # HNSW parameter: Higher values = more accurate but slower search
  max_connections: 64  # HNSW parameter: Number of connections per node
  ef_construction: 128  # HNSW parameter: Higher values = more accurate index
  vector_cache_max_objects: 1000000  # Maximum number of vectors to cache
  distance_metric: "cosine"  # cosine, dot, l2-squared
  index_quantizer_enabled: true  # Enable vector quantization for faster search

# Preprocessing Configuration
preprocessing:
  csv_chunk_size: 10000  # Number of rows to process at once
  normalize_strings: true  # Normalize strings (lowercase, remove extra whitespace)
  deduplication_enabled: true  # Enable deduplication of strings
  null_values: ["NULL", "null", "", "None", "NA", "N/A"]  # Values to treat as null

# Imputation Configuration
imputation:
  fields_to_impute:
    - "provision"
    - "subjects"
  vector_similarity_threshold: 0.30  # Minimum similarity for imputation
  max_candidates: 10  # Number of candidates to consider for imputation
  imputation_method: "average"  # average, weighted_average, nearest

# Feature Engineering
features:
  # Direct similarity features
  cosine_similarities:
    - "person"
    - "title"
    - "provision"
    - "subjects"
    - "composite"
  
  # String similarities
  string_similarities:
    - 
      field: "person"
      metrics: ["levenshtein", "jaro_winkler"]
  
  # Interaction features (harmonic means)
  harmonic_means:
    - ["person", "title"]
    - ["person", "provision"]
    - ["person", "subjects"]
    - ["title", "subjects"]
    - ["title", "provision"]
    - ["provision", "subjects"]
  
  # Additional interaction features
  additional_interactions:
    - 
      type: "product"
      fields: ["person", "subjects"]
    - 
      type: "ratio"
      fields: ["composite", "subjects"]
  
  # Feature normalization
  normalize_features: true
  
  # Recursive feature elimination
  rfe_enabled: true
  rfe_step_size: 1
  rfe_cv_folds: 5
  
  # Prefilterers for automatic classification
  prefilters:
    exact_name_birth_death_prefilter: true
    composite_cosine_prefilter:
      enabled: true
      threshold: 0.65
    person_cosine_prefilter:
      enabled: true
      threshold: 0.70

# Classification Configuration
classification:
  algorithm: "logistic_regression"
  regularization: "l2"
  regularization_strength: 1.0
  learning_rate: 0.01
  max_iterations: 1000
  convergence_tolerance: 0.0001
  batch_size: 1000
  class_weight: "balanced"  # Options: None, "balanced"
  decision_threshold: 0.95
  precision_recall_tradeoff: "balanced"  # precision, recall, balanced, f1

# Clustering Configuration
clustering:
  algorithm: "connected_components"  # connected_components, louvain, label_propagation
  min_edge_weight: 0.5  # Minimum classifier confidence for an edge
  transitivity_enabled: true
  resolve_conflicts: true
  min_cluster_size: 1

# Monitoring and Metrics
monitoring:
  prometheus_enabled: true
  prometheus_port: 8000
  metrics_update_interval: 5  # Seconds
  log_progress: true
  progress_interval: 1000  # Log progress every N records

# Reporting and Analysis
reporting:
  generate_metrics_report: true
  generate_feature_importance: true
  generate_error_analysis: true
  generate_cluster_statistics: true
  visualization_enabled: true
  metrics_to_report:
    - "precision"
    - "recall"
    - "f1"
    - "accuracy"
    - "roc_auc"
    - "confusion_matrix"
